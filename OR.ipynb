{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "mount_file_id": "15xK2rGgMvhBhUfGDxJ91VstsdwWEpM7h",
      "authorship_tag": "ABX9TyP5DssFE/gzifxuSKVieV/E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShankyMalewar/Asklyne_AI_Backend/blob/main/OR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAcqFW35DXh3",
        "outputId": "cfb6297c-ff27-462d-82ae-70e23c1b2bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (2.9.0.post0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Colab cell 1\n",
        "!pip install pandas networkx tqdm python-dateutil\n",
        "\n",
        "# If you have Gurobi and license available in Colab uncomment:\n",
        "# !pip install gurobipy\n",
        "\n",
        "import os, glob, math\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from datetime import datetime, timedelta\n",
        "from dateutil import parser\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8154085e"
      },
      "source": [
        "mkdir /content/instance1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Robust loader for listOfBases + day_x files\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "from dateutil import parser\n",
        "from tqdm import tqdm\n",
        "\n",
        "INSTANCE_DIR = \"/content/instance1\"  # update if needed\n",
        "\n",
        "# --- 1) Load bases robustly ---\n",
        "bases_path = os.path.join(INSTANCE_DIR, \"listOfBases.csv\")\n",
        "bases_df = pd.read_csv(bases_path)\n",
        "# normalize column names\n",
        "bases_df.columns = bases_df.columns.str.strip().str.lower()\n",
        "# common name variants mapping\n",
        "colmap_bases = {}\n",
        "if 'airport' in bases_df.columns:\n",
        "    colmap_bases['airport'] = 'airport'\n",
        "elif 'airports' in bases_df.columns:\n",
        "    colmap_bases['airport'] = 'airports'\n",
        "# status variants\n",
        "for c in ['status','stat','is_base']:\n",
        "    if c in bases_df.columns:\n",
        "        colmap_bases['status'] = c\n",
        "        break\n",
        "# nbEmployees variants\n",
        "for c in ['nbemployees','nbEmployees','nb_employees','nbemployees']:\n",
        "    if c in bases_df.columns:\n",
        "        colmap_bases['nbemployees'] = c\n",
        "        break\n",
        "\n",
        "# If mapping incomplete, try fuzzy matching:\n",
        "if 'airport' not in colmap_bases:\n",
        "    for c in bases_df.columns:\n",
        "        if 'air' in c and 'port' in c:\n",
        "            colmap_bases['airport'] = c; break\n",
        "if 'status' not in colmap_bases:\n",
        "    for c in bases_df.columns:\n",
        "        if 'stat' in c:\n",
        "            colmap_bases['status'] = c; break\n",
        "if 'nbemployees' not in colmap_bases:\n",
        "    for c in bases_df.columns:\n",
        "        if 'nb' in c or 'employee' in c:\n",
        "            colmap_bases['nbemployees'] = c; break\n",
        "\n",
        "# rename for internal use\n",
        "bases_df = bases_df.rename(columns={colmap_bases['airport']:'airport',\n",
        "                                    colmap_bases['status']:'status',\n",
        "                                    colmap_bases['nbemployees']:'nbemployees'})\n",
        "# strip whitespace\n",
        "bases_df['airport'] = bases_df['airport'].astype(str).str.strip()\n",
        "bases_df['status'] = bases_df['status'].astype(str).str.strip()\n",
        "# convert status to int safely\n",
        "bases_df['status'] = bases_df['status'].replace('', '0')  # default if missing\n",
        "bases_df['status'] = bases_df['status'].astype(int)\n",
        "print(\"Bases head (clean):\")\n",
        "print(bases_df.head())\n",
        "bases = bases_df[bases_df['status'] == 1]['airport'].tolist()\n",
        "print(\"Bases (status==1):\", len(bases), \"example:\", bases[:8])\n",
        "\n",
        "# --- 2) Load day_*.csv files robustly and build flights_df ---\n",
        "files = sorted(glob.glob(os.path.join(INSTANCE_DIR, \"day_*.csv\")))\n",
        "print(\"Found day files:\", files)\n",
        "\n",
        "flights = []\n",
        "bad_files = []\n",
        "expected_keys = [\"leg\", \"dep_air\", \"dep_date\", \"dep_time\", \"arr_air\", \"arr_date\", \"arr_time\"]\n",
        "# function to find best matching column by keyword\n",
        "def find_col(cols, keywords):\n",
        "    for k in keywords:\n",
        "        for c in cols:\n",
        "            if k in c:\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "for fpath in files:\n",
        "    try:\n",
        "        df = pd.read_csv(fpath)\n",
        "        orig_cols = list(df.columns)\n",
        "        # normalize columns: strip and lower\n",
        "        df.columns = [c.strip().lower() for c in df.columns.astype(str)]\n",
        "        cols = df.columns.tolist()\n",
        "\n",
        "        # try to map columns\n",
        "        # leg id variants\n",
        "        leg_col = find_col(cols, ['leg','flight','leg_nb','legno','leg no','leg#'])\n",
        "        dep_air_col = find_col(cols, ['airport_dep','dep_airport','dep','origin','airport_dep'])\n",
        "        dep_date_col = find_col(cols, ['date_dep','dep_date','date dep','dep_date'])\n",
        "        dep_time_col = find_col(cols, ['hour_dep','dep_time','time_dep','dep_time','hour dep'])\n",
        "        arr_air_col = find_col(cols, ['airport_arr','arr_airport','arr','destination','airport_arr'])\n",
        "        arr_date_col = find_col(cols, ['date_arr','arr_date','date arr'])\n",
        "        arr_time_col = find_col(cols, ['hour_arr','arr_time','time_arr','arr_time','hour arr'])\n",
        "\n",
        "        mapping = {\n",
        "            'leg': leg_col, 'dep_air': dep_air_col, 'dep_date': dep_date_col,\n",
        "            'dep_time': dep_time_col, 'arr_air': arr_air_col, 'arr_date': arr_date_col, 'arr_time': arr_time_col\n",
        "        }\n",
        "        if any(v is None for v in mapping.values()):\n",
        "            bad_files.append((fpath, orig_cols))\n",
        "            continue\n",
        "\n",
        "        # iterate rows\n",
        "        for _, r in df.iterrows():\n",
        "            leg = r[leg_col]\n",
        "            dep_air = str(r[dep_air_col]).strip()\n",
        "            arr_air = str(r[arr_air_col]).strip()\n",
        "            dep_date = str(r[dep_date_col]).strip()\n",
        "            dep_time = str(r[dep_time_col]).strip()\n",
        "            arr_date = str(r[arr_date_col]).strip()\n",
        "            arr_time = str(r[arr_time_col]).strip()\n",
        "            # some weird blanks -> skip\n",
        "            if dep_date == '' or dep_time == '' or arr_date == '' or arr_time == '':\n",
        "                continue\n",
        "            try:\n",
        "                dep_dt = parser.parse(dep_date + \" \" + dep_time)\n",
        "                arr_dt = parser.parse(arr_date + \" \" + arr_time)\n",
        "            except Exception as e:\n",
        "                # try with replacement for weird formats\n",
        "                try:\n",
        "                    dep_dt = parser.parse(dep_date + \" \" + dep_time.replace('.',':'))\n",
        "                    arr_dt = parser.parse(arr_date + \" \" + arr_time.replace('.',':'))\n",
        "                except Exception:\n",
        "                    continue\n",
        "            flights.append({\n",
        "                \"flight_id\": str(leg).strip(),\n",
        "                \"dep_airport\": dep_air,\n",
        "                \"arr_airport\": arr_air,\n",
        "                \"dep_dt\": dep_dt,\n",
        "                \"arr_dt\": arr_dt,\n",
        "                \"duration_hr\": (arr_dt - dep_dt).total_seconds()/3600.0\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        bad_files.append((fpath, str(e)))\n",
        "\n",
        "flights_df = pd.DataFrame(flights)\n",
        "flights_df = flights_df.sort_values(['dep_dt']).reset_index(drop=True)\n",
        "print(\"Total flights loaded:\", len(flights_df))\n",
        "if bad_files:\n",
        "    print(\"Warning: some files had unexpected schema or errors. Inspect below:\")\n",
        "    for bf in bad_files:\n",
        "        print(bf[0], \" -> cols/or error:\", bf[1])\n",
        "\n",
        "# quick peek\n",
        "flights_df.head(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "tCKyu8A5D4td",
        "outputId": "6e0f4b0d-42eb-4f8c-8028-6a0a6a3d9be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bases head (clean):\n",
            "  airport  status  nbemployees\n",
            "0   BASE1       1            7\n",
            "1    AIR1       0            0\n",
            "2   BASE2       1           20\n",
            "3    AIR2       0            0\n",
            "4    AIR3       0            0\n",
            "Bases (status==1): 3 example: ['BASE1', 'BASE2', 'BASE3']\n",
            "Found day files: ['/content/instance1/day_1.csv', '/content/instance1/day_10.csv', '/content/instance1/day_11.csv', '/content/instance1/day_12.csv', '/content/instance1/day_13.csv', '/content/instance1/day_14.csv', '/content/instance1/day_15.csv', '/content/instance1/day_16.csv', '/content/instance1/day_17.csv', '/content/instance1/day_18.csv', '/content/instance1/day_19.csv', '/content/instance1/day_2.csv', '/content/instance1/day_20.csv', '/content/instance1/day_21.csv', '/content/instance1/day_22.csv', '/content/instance1/day_23.csv', '/content/instance1/day_24.csv', '/content/instance1/day_25.csv', '/content/instance1/day_26.csv', '/content/instance1/day_27.csv', '/content/instance1/day_28.csv', '/content/instance1/day_29.csv', '/content/instance1/day_3.csv', '/content/instance1/day_30.csv', '/content/instance1/day_31.csv', '/content/instance1/day_4.csv', '/content/instance1/day_5.csv', '/content/instance1/day_6.csv', '/content/instance1/day_7.csv', '/content/instance1/day_8.csv', '/content/instance1/day_9.csv']\n",
            "Total flights loaded: 1013\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   flight_id dep_airport arr_airport              dep_dt              arr_dt  \\\n",
              "0   LEG_01_0       BASE1        AIR1 2000-01-01 12:00:00 2000-01-01 13:13:00   \n",
              "1  LEG_01_30       BASE2       AIR12 2000-01-01 12:59:00 2000-01-01 15:46:00   \n",
              "2   LEG_01_1        AIR1       BASE2 2000-01-01 14:05:00 2000-01-01 15:19:00   \n",
              "3  LEG_01_18       BASE3        AIR9 2000-01-01 14:05:00 2000-01-01 15:55:00   \n",
              "4   LEG_01_6       BASE2        AIR6 2000-01-01 14:21:00 2000-01-01 16:54:00   \n",
              "5  LEG_01_11       BASE1        AIR9 2000-01-01 15:24:00 2000-01-01 18:32:00   \n",
              "6  LEG_01_31       BASE2        AIR3 2000-01-01 16:13:00 2000-01-01 16:59:00   \n",
              "7  LEG_01_28       AIR12       BASE2 2000-01-01 17:00:00 2000-01-01 19:52:00   \n",
              "8  LEG_01_15        AIR9       BASE2 2000-01-01 17:05:00 2000-01-01 19:44:00   \n",
              "9   LEG_01_8        AIR6       BASE2 2000-01-01 17:40:00 2000-01-01 20:23:00   \n",
              "\n",
              "   duration_hr  \n",
              "0     1.216667  \n",
              "1     2.783333  \n",
              "2     1.233333  \n",
              "3     1.833333  \n",
              "4     2.550000  \n",
              "5     3.133333  \n",
              "6     0.766667  \n",
              "7     2.866667  \n",
              "8     2.650000  \n",
              "9     2.716667  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a86a7c1-53cd-4274-97b3-73fbb9bfd796\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>flight_id</th>\n",
              "      <th>dep_airport</th>\n",
              "      <th>arr_airport</th>\n",
              "      <th>dep_dt</th>\n",
              "      <th>arr_dt</th>\n",
              "      <th>duration_hr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LEG_01_0</td>\n",
              "      <td>BASE1</td>\n",
              "      <td>AIR1</td>\n",
              "      <td>2000-01-01 12:00:00</td>\n",
              "      <td>2000-01-01 13:13:00</td>\n",
              "      <td>1.216667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LEG_01_30</td>\n",
              "      <td>BASE2</td>\n",
              "      <td>AIR12</td>\n",
              "      <td>2000-01-01 12:59:00</td>\n",
              "      <td>2000-01-01 15:46:00</td>\n",
              "      <td>2.783333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LEG_01_1</td>\n",
              "      <td>AIR1</td>\n",
              "      <td>BASE2</td>\n",
              "      <td>2000-01-01 14:05:00</td>\n",
              "      <td>2000-01-01 15:19:00</td>\n",
              "      <td>1.233333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LEG_01_18</td>\n",
              "      <td>BASE3</td>\n",
              "      <td>AIR9</td>\n",
              "      <td>2000-01-01 14:05:00</td>\n",
              "      <td>2000-01-01 15:55:00</td>\n",
              "      <td>1.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LEG_01_6</td>\n",
              "      <td>BASE2</td>\n",
              "      <td>AIR6</td>\n",
              "      <td>2000-01-01 14:21:00</td>\n",
              "      <td>2000-01-01 16:54:00</td>\n",
              "      <td>2.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LEG_01_11</td>\n",
              "      <td>BASE1</td>\n",
              "      <td>AIR9</td>\n",
              "      <td>2000-01-01 15:24:00</td>\n",
              "      <td>2000-01-01 18:32:00</td>\n",
              "      <td>3.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LEG_01_31</td>\n",
              "      <td>BASE2</td>\n",
              "      <td>AIR3</td>\n",
              "      <td>2000-01-01 16:13:00</td>\n",
              "      <td>2000-01-01 16:59:00</td>\n",
              "      <td>0.766667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>LEG_01_28</td>\n",
              "      <td>AIR12</td>\n",
              "      <td>BASE2</td>\n",
              "      <td>2000-01-01 17:00:00</td>\n",
              "      <td>2000-01-01 19:52:00</td>\n",
              "      <td>2.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>LEG_01_15</td>\n",
              "      <td>AIR9</td>\n",
              "      <td>BASE2</td>\n",
              "      <td>2000-01-01 17:05:00</td>\n",
              "      <td>2000-01-01 19:44:00</td>\n",
              "      <td>2.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>LEG_01_8</td>\n",
              "      <td>AIR6</td>\n",
              "      <td>BASE2</td>\n",
              "      <td>2000-01-01 17:40:00</td>\n",
              "      <td>2000-01-01 20:23:00</td>\n",
              "      <td>2.716667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a86a7c1-53cd-4274-97b3-73fbb9bfd796')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7a86a7c1-53cd-4274-97b3-73fbb9bfd796 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7a86a7c1-53cd-4274-97b3-73fbb9bfd796');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1a5fa0a4-3b64-4f68-9d0e-d5f8782ede93\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1a5fa0a4-3b64-4f68-9d0e-d5f8782ede93')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1a5fa0a4-3b64-4f68-9d0e-d5f8782ede93 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "flights_df",
              "summary": "{\n  \"name\": \"flights_df\",\n  \"rows\": 1013,\n  \"fields\": [\n    {\n      \"column\": \"flight_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1013,\n        \"samples\": [\n          \"LEG_21_28\",\n          \"LEG_16_2\",\n          \"LEG_29_2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dep_airport\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"AIR2\",\n          \"AIR14\",\n          \"BASE1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"arr_airport\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"BASE3\",\n          \"AIR14\",\n          \"AIR1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dep_dt\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2000-01-01 12:00:00\",\n        \"max\": \"2000-01-31 23:40:00\",\n        \"num_unique_values\": 1006,\n        \"samples\": [\n          \"2000-01-29 02:22:00\",\n          \"2000-01-19 19:20:00\",\n          \"2000-01-21 17:40:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"arr_dt\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2000-01-01 13:13:00\",\n        \"max\": \"2000-02-01 01:50:00\",\n        \"num_unique_values\": 996,\n        \"samples\": [\n          \"2000-01-26 13:26:00\",\n          \"2000-01-31 01:13:00\",\n          \"2000-01-04 00:09:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duration_hr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7813708186532176,\n        \"min\": 0.6166666666666667,\n        \"max\": 3.2666666666666666,\n        \"num_unique_values\": 69,\n        \"samples\": [\n          1.1833333333333333,\n          1.2166666666666666,\n          2.033333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Next step: build adjacency, enumerate pairings, compute cost, save ===\n",
        "import time, json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Parameters (tune if needed)\n",
        "MIN_CONNECT_MIN = 45         # minutes\n",
        "MIN_REST_HOURS = 10          # overnight rest threshold (new duty)\n",
        "MAX_PAIRING_HOURS = 84       # total hours allowed from first dep to last arr\n",
        "MAX_PAIRING_FLIGHTS = 12     # max legs per pairing\n",
        "MAX_PAIRING_DUTIES = 4       # max number of duties (rest cycles)\n",
        "MAX_ENUM_PAIRINGS = 200000   # safety cap to abort enumeration if too many\n",
        "\n",
        "# Precompute useful arrays to speed checks\n",
        "n = len(flights_df)\n",
        "dep_air = flights_df['dep_airport'].to_list()\n",
        "arr_air = flights_df['arr_airport'].to_list()\n",
        "dep_dt  = flights_df['dep_dt'].to_list()\n",
        "arr_dt  = flights_df['arr_dt'].to_list()\n",
        "\n",
        "# Build adjacency using time-windowed search (faster than nested loops)\n",
        "# For each flight i, find candidate j where dep_dt[j] > arr_dt[i] and airports match\n",
        "from bisect import bisect_left\n",
        "# Create index of flights sorted by dep_dt for range checks\n",
        "sorted_by_dep = flights_df.sort_values('dep_dt').reset_index()\n",
        "dep_sorted_idx = sorted_by_dep['index'].to_list()\n",
        "dep_times_sorted = sorted_by_dep['dep_dt'].to_list()\n",
        "\n",
        "def can_connect_idx(i, j):\n",
        "    # check airports and time gap\n",
        "    if arr_air[i] != dep_air[j]:\n",
        "        return False\n",
        "    gap_min = (dep_dt[j] - arr_dt[i]).total_seconds()/60.0\n",
        "    if gap_min < MIN_CONNECT_MIN or gap_min < 0:\n",
        "        return False\n",
        "    # avoid ridiculously long same-duty gaps\n",
        "    if gap_min/60.0 > 72:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Build adjacency lists\n",
        "adj = [[] for _ in range(n)]\n",
        "# For each i, find earliest candidate j in sorted_by_dep where dep_dt[j] >= arr_dt[i] + MIN_CONNECT\n",
        "for i in tqdm(range(n), desc=\"building adjacency\"):\n",
        "    earliest_dt = arr_dt[i] + pd.Timedelta(minutes=MIN_CONNECT_MIN)\n",
        "    # find first candidate index in dep_times_sorted\n",
        "    pos = bisect_left(dep_times_sorted, earliest_dt)\n",
        "    # iterate forward from pos while within a reasonable window (arr_dt + 72h)\n",
        "    latest_dt = arr_dt[i] + pd.Timedelta(hours=72)\n",
        "    k = pos\n",
        "    while k < len(dep_times_sorted) and dep_times_sorted[k] <= latest_dt:\n",
        "        j = dep_sorted_idx[k]  # original index\n",
        "        if can_connect_idx(i, j):\n",
        "            adj[i].append(j)\n",
        "        k += 1\n",
        "\n",
        "# Quick adjacency stats\n",
        "num_edges = sum(len(v) for v in adj)\n",
        "print(\"Adjacency built: flights n =\", n, \", edges =\", num_edges)\n",
        "\n",
        "# === Enumerate pairings with DFS, bounded by constraints ===\n",
        "pairings = []\n",
        "abort = False\n",
        "start_time = time.time()\n",
        "\n",
        "def estimate_duties_from_path(path_idx):\n",
        "    duties = 1\n",
        "    for a,b in zip(path_idx[:-1], path_idx[1:]):\n",
        "        gap_h = (dep_dt[b] - arr_dt[a]).total_seconds()/3600.0\n",
        "        if gap_h >= MIN_REST_HOURS:\n",
        "            duties += 1\n",
        "    return duties\n",
        "\n",
        "def dfs(path):\n",
        "    global abort, pairings\n",
        "    if abort:\n",
        "        return\n",
        "    first_idx = path[0]; last_idx = path[-1]\n",
        "    total_hours = (arr_dt[last_idx] - dep_dt[first_idx]).total_seconds()/3600.0\n",
        "    if total_hours > MAX_PAIRING_HOURS or len(path) > MAX_PAIRING_FLIGHTS:\n",
        "        return\n",
        "    duties = estimate_duties_from_path(path)\n",
        "    if duties <= MAX_PAIRING_DUTIES:\n",
        "        # pairing must start & end at same base\n",
        "        start_base = dep_air[first_idx].strip()\n",
        "        end_base   = arr_air[last_idx].strip()\n",
        "        if start_base == end_base and start_base in bases:\n",
        "            pairings.append({\n",
        "                \"flights_idx\": path.copy(),\n",
        "                \"start_base\": start_base,\n",
        "                \"first_dep\": dep_dt[first_idx].isoformat(),\n",
        "                \"last_arr\": arr_dt[last_idx].isoformat(),\n",
        "                \"total_hours\": total_hours,\n",
        "                \"duties\": duties,\n",
        "                \"n_legs\": len(path)\n",
        "            })\n",
        "            # safety check\n",
        "            if len(pairings) >= MAX_ENUM_PAIRINGS:\n",
        "                abort = True\n",
        "                return\n",
        "    # expand\n",
        "    for succ in adj[last_idx]:\n",
        "        if succ in path:\n",
        "            continue\n",
        "        # cheap prune: if succ departure is more than MAX_PAIRING_HOURS after first dep, skip\n",
        "        if (dep_dt[succ] - dep_dt[first_idx]).total_seconds()/3600.0 > MAX_PAIRING_HOURS:\n",
        "            continue\n",
        "        dfs(path + [succ])\n",
        "\n",
        "# Start DFS from flights that depart from a base\n",
        "starts = [i for i in range(n) if dep_air[i].strip() in bases]\n",
        "print(\"Starting DFS from\", len(starts), \"base-departure flights\")\n",
        "for s in tqdm(starts, desc=\"enumerating pairings\"):\n",
        "    dfs([s])\n",
        "    if abort:\n",
        "        break\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"Enumeration finished in {elapsed:.1f}s, pairings found = {len(pairings)}, aborted={abort}\")\n",
        "\n",
        "# Convert to DataFrame, compute cost & credit\n",
        "pairings_df = pd.DataFrame(pairings)\n",
        "if pairings_df.shape[0] == 0:\n",
        "    print(\"No pairings found — adjust MIN_REST_HOURS / MAX_PAIRING_HOURS or check bases mapping.\")\n",
        "else:\n",
        "    # compute credit & cost (adjust weights as needed)\n",
        "    def pairing_cost(row, pay_per_hour=100.0, overnight_cost=200.0):\n",
        "        return row['total_hours'] * pay_per_hour + row['duties'] * overnight_cost\n",
        "    pairings_df['credit'] = pairings_df['total_hours']\n",
        "    pairings_df['cost'] = pairings_df.apply(pairing_cost, axis=1)\n",
        "    print(pairings_df[['start_base','n_legs','duties','total_hours','cost']].describe())\n",
        "    display(pairings_df.head(10))\n",
        "\n",
        "# Save results for later solver steps\n",
        "# === Save pairings safely (fixed) ===\n",
        "import pickle, json, os\n",
        "\n",
        "# save pairings and flights\n",
        "pairings_pkl = \"/content/pairings_instance1.pkl\"\n",
        "flights_pkl  = \"/content/flights_instance1.pkl\"\n",
        "pairings_df.to_pickle(pairings_pkl)\n",
        "flights_df.to_pickle(flights_pkl)\n",
        "print(\"Saved pickles:\", pairings_pkl, flights_pkl)\n",
        "\n",
        "# build A: flight_idx (str) -> list(pairing_idx)\n",
        "A = {}\n",
        "for p_idx, row in pairings_df.iterrows():\n",
        "    for f in row['flights_idx']:\n",
        "        key = str(int(f))    # ensure json-friendly string keys\n",
        "        A.setdefault(key, []).append(int(p_idx))\n",
        "\n",
        "# build B: base -> list(pairing_idx)\n",
        "B = {}\n",
        "for p_idx, row in pairings_df.iterrows():\n",
        "    base = str(row['start_base']).strip()\n",
        "    B.setdefault(base, []).append(int(p_idx))\n",
        "\n",
        "# save indices as JSON\n",
        "with open(\"/content/A_index.json\",\"w\") as fh:\n",
        "    json.dump(A, fh)\n",
        "with open(\"/content/B_index.json\",\"w\") as fh:\n",
        "    json.dump(B, fh)\n",
        "\n",
        "print(\"Saved A_index.json and B_index.json (flight->pairings and base->pairings)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "T3brAQSdFb0e",
        "outputId": "ee8ba894-6f0e-4405-c29e-ca2b88800d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "building adjacency: 100%|██████████| 1013/1013 [00:00<00:00, 9950.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjacency built: flights n = 1013 , edges = 14729\n",
            "Starting DFS from 509 base-departure flights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "enumerating pairings:   1%|          | 3/509 [01:37<4:35:14, 32.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enumeration finished in 97.9s, pairings found = 200000, aborted=True\n",
            "              n_legs         duties    total_hours           cost\n",
            "count  200000.000000  200000.000000  200000.000000  200000.000000\n",
            "mean       10.867450       3.547040      78.712437    8580.651700\n",
            "std         1.506971       0.607708       5.902027     638.671535\n",
            "min         2.000000       1.000000       6.033333     803.333333\n",
            "25%        10.000000       3.000000      77.516667    8540.000000\n",
            "50%        12.000000       4.000000      80.866667    8721.666667\n",
            "75%        12.000000       4.000000      80.900000    8890.000000\n",
            "max        12.000000       4.000000      83.566667    9156.666667\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       flights_idx start_base  \\\n",
              "0    [0, 2, 6, 10, 15, 18, 26, 32, 40, 45, 63, 69]      BASE1   \n",
              "1   [0, 2, 6, 10, 15, 18, 26, 32, 40, 45, 63, 105]      BASE1   \n",
              "2    [0, 2, 6, 10, 15, 18, 26, 32, 40, 45, 83, 88]      BASE1   \n",
              "3   [0, 2, 6, 10, 15, 18, 26, 32, 40, 45, 83, 124]      BASE1   \n",
              "4   [0, 2, 6, 10, 15, 18, 26, 32, 40, 45, 87, 124]      BASE1   \n",
              "5   [0, 2, 6, 10, 15, 18, 26, 32, 40, 45, 99, 105]      BASE1   \n",
              "6  [0, 2, 6, 10, 15, 18, 26, 32, 40, 45, 119, 124]      BASE1   \n",
              "7    [0, 2, 6, 10, 15, 18, 26, 32, 40, 73, 83, 88]      BASE1   \n",
              "8   [0, 2, 6, 10, 15, 18, 26, 32, 40, 73, 83, 124]      BASE1   \n",
              "9   [0, 2, 6, 10, 15, 18, 26, 32, 40, 73, 87, 124]      BASE1   \n",
              "\n",
              "             first_dep             last_arr  total_hours  duties  n_legs  \\\n",
              "0  2000-01-01T12:00:00  2000-01-03T14:30:00         50.5       3      12   \n",
              "1  2000-01-01T12:00:00  2000-01-04T14:30:00         74.5       3      12   \n",
              "2  2000-01-01T12:00:00  2000-01-03T20:54:00         56.9       3      12   \n",
              "3  2000-01-01T12:00:00  2000-01-04T20:54:00         80.9       4      12   \n",
              "4  2000-01-01T12:00:00  2000-01-04T20:54:00         80.9       4      12   \n",
              "5  2000-01-01T12:00:00  2000-01-04T14:30:00         74.5       4      12   \n",
              "6  2000-01-01T12:00:00  2000-01-04T20:54:00         80.9       3      12   \n",
              "7  2000-01-01T12:00:00  2000-01-03T20:54:00         56.9       3      12   \n",
              "8  2000-01-01T12:00:00  2000-01-04T20:54:00         80.9       4      12   \n",
              "9  2000-01-01T12:00:00  2000-01-04T20:54:00         80.9       4      12   \n",
              "\n",
              "   credit    cost  \n",
              "0    50.5  5650.0  \n",
              "1    74.5  8050.0  \n",
              "2    56.9  6290.0  \n",
              "3    80.9  8890.0  \n",
              "4    80.9  8890.0  \n",
              "5    74.5  8250.0  \n",
              "6    80.9  8690.0  \n",
              "7    56.9  6290.0  \n",
              "8    80.9  8890.0  \n",
              "9    80.9  8890.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9e93bb03-2446-4cd5-9b2d-137abe0e1e16\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>flights_idx</th>\n",
              "      <th>start_base</th>\n",
              "      <th>first_dep</th>\n",
              "      <th>last_arr</th>\n",
              "      <th>total_hours</th>\n",
              "      <th>duties</th>\n",
              "      <th>n_legs</th>\n",
              "      <th>credit</th>\n",
              "      <th>cost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0, 2, 6, 10, 15, 18, 26, 32, 40, 45, 63, 69]</td>\n",
              "      <td>BASE1</td>\n",
              "      <td>2000-01-01T12:00:00</td>\n",
              "      <td>2000-01-03T14:30:00</td>\n",
              "      <td>50.5</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>50.5</td>\n",
              "      <td>5650.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0, 2, 6, 10, 15, 18, 26, 32, 40, 45, 63, 105]</td>\n",
              "      <td>BASE1</td>\n",
              "      <td>2000-01-01T12:00:00</td>\n",
              "      <td>2000-01-04T14:30:00</td>\n",
              "      <td>74.5</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>74.5</td>\n",
              "      <td>8050.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0, 2, 6, 10, 15, 18, 26, 32, 40, 45, 83, 88]</td>\n",
              "      <td>BASE1</td>\n",
              "      <td>2000-01-01T12:00:00</td>\n",
              "      <td>2000-01-03T20:54:00</td>\n",
              "      <td>56.9</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>56.9</td>\n",
              "      <td>6290.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0, 2, 6, 10, 15, 18, 26, 32, 40, 45, 83, 124]</td>\n",
              "      <td>BASE1</td>\n",
              "      <td>2000-01-01T12:00:00</td>\n",
              "      <td>2000-01-04T20:54:00</td>\n",
              "      <td>80.9</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>80.9</td>\n",
              "      <td>8890.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0, 2, 6, 10, 15, 18, 26, 32, 40, 45, 87, 124]</td>\n",
              "      <td>BASE1</td>\n",
              "      <td>2000-01-01T12:00:00</td>\n",
              "      <td>2000-01-04T20:54:00</td>\n",
              "      <td>80.9</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>80.9</td>\n",
              "      <td>8890.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[0, 2, 6, 10, 15, 18, 26, 32, 40, 45, 99, 105]</td>\n",
              "      <td>BASE1</td>\n",
              "      <td>2000-01-01T12:00:00</td>\n",
              "      <td>2000-01-04T14:30:00</td>\n",
              "      <td>74.5</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>74.5</td>\n",
              "      <td>8250.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[0, 2, 6, 10, 15, 18, 26, 32, 40, 45, 119, 124]</td>\n",
              "      <td>BASE1</td>\n",
              "      <td>2000-01-01T12:00:00</td>\n",
              "      <td>2000-01-04T20:54:00</td>\n",
              "      <td>80.9</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>80.9</td>\n",
              "      <td>8690.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[0, 2, 6, 10, 15, 18, 26, 32, 40, 73, 83, 88]</td>\n",
              "      <td>BASE1</td>\n",
              "      <td>2000-01-01T12:00:00</td>\n",
              "      <td>2000-01-03T20:54:00</td>\n",
              "      <td>56.9</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>56.9</td>\n",
              "      <td>6290.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[0, 2, 6, 10, 15, 18, 26, 32, 40, 73, 83, 124]</td>\n",
              "      <td>BASE1</td>\n",
              "      <td>2000-01-01T12:00:00</td>\n",
              "      <td>2000-01-04T20:54:00</td>\n",
              "      <td>80.9</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>80.9</td>\n",
              "      <td>8890.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[0, 2, 6, 10, 15, 18, 26, 32, 40, 73, 87, 124]</td>\n",
              "      <td>BASE1</td>\n",
              "      <td>2000-01-01T12:00:00</td>\n",
              "      <td>2000-01-04T20:54:00</td>\n",
              "      <td>80.9</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>80.9</td>\n",
              "      <td>8890.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e93bb03-2446-4cd5-9b2d-137abe0e1e16')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9e93bb03-2446-4cd5-9b2d-137abe0e1e16 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9e93bb03-2446-4cd5-9b2d-137abe0e1e16');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4f8049aa-c57f-4623-bd55-dac222c213c0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4f8049aa-c57f-4623-bd55-dac222c213c0')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4f8049aa-c57f-4623-bd55-dac222c213c0 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"Saved A_index\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"flights_idx\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start_base\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"BASE1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"first_dep\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2000-01-01T12:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"last_arr\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"2000-01-04T14:30:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_hours\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.13963206471542,\n        \"min\": 50.5,\n        \"max\": 80.9,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          74.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duties\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 4,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_legs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 12,\n        \"max\": 12,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          12\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"credit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.13963206471542,\n        \"min\": 50.5,\n        \"max\": 80.9,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          74.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cost\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1288.0372665416169,\n        \"min\": 5650.0,\n        \"max\": 8890.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          5650.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pickles: /content/pairings_instance1.pkl /content/flights_instance1.pkl\n",
            "Saved A_index.json and B_index.json (flight->pairings and base->pairings)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()   # Choose your gurobi.lic file\n",
        "\n",
        "# Move it to where Gurobi expects it\n",
        "!mkdir -p /root/gurobi\n",
        "!mv gurobi.lic /root/gurobi/\n",
        "!export GRB_LICENSE_FILE=/root/gurobi/gurobi.lic\n",
        "\n",
        "# Verify license\n",
        "!pip install -q gurobipy\n",
        "from gurobipy import Model, GRB\n",
        "m = Model(\"license_test\")\n",
        "x = m.addVar(name=\"x\")\n",
        "m.setObjective(x, GRB.MAXIMIZE)\n",
        "m.optimize()\n",
        "print(\"✅ Gurobi license file loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "ZTmPvFJ0LWqF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "03247228-fc82-450f-9b34-eb2f1bf9d1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-504e7a1a-4565-4bbd-b383-a8b31d006124\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-504e7a1a-4565-4bbd-b383-a8b31d006124\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Prune pairings per flight and run solve_cpp in a loop (auto-shrink K if Gurobi license too small) ===\n",
        "import os, json, math, shutil, sys, time\n",
        "import pandas as pd\n",
        "\n",
        "INSTANCE_DIR = \"/content/instance1\"   # adjust if needed\n",
        "PAIRINGS_PKL = \"/content/pairings_instance1.pkl\"\n",
        "FLIGHTS_PKL  = \"/content/flights_instance1.pkl\"\n",
        "A_INDEX_JSON = \"/content/A_index.json\"\n",
        "LIST_OF_BASES = os.path.join(INSTANCE_DIR, \"listOfBases.csv\")\n",
        "\n",
        "# sanity checks\n",
        "for p in [PAIRINGS_PKL, FLIGHTS_PKL, A_INDEX_JSON, LIST_OF_BASES]:\n",
        "    if not os.path.exists(p):\n",
        "        raise FileNotFoundError(f\"Required file not found: {p}\")\n",
        "\n",
        "pairings_df = pd.read_pickle(PAIRINGS_PKL).reset_index(drop=True)\n",
        "flights_df  = pd.read_pickle(FLIGHTS_PKL).reset_index(drop=True)\n",
        "with open(A_INDEX_JSON, \"r\") as fh:\n",
        "    A_index = json.load(fh)\n",
        "\n",
        "# make sure cost exists\n",
        "if 'cost' not in pairings_df.columns:\n",
        "    PAY_PER_HOUR = 100.0\n",
        "    OVERNIGHT_COST = 200.0\n",
        "    pairings_df['cost'] = pairings_df.get('total_hours', pairings_df.get('duration_hr', 0.0)) * PAY_PER_HOUR + pairings_df.get('duties', 0) * OVERNIGHT_COST\n",
        "\n",
        "# Ensure pairing_id exists (original index-based)\n",
        "pairings_df['orig_index'] = pairings_df.index.astype(int)\n",
        "pairings_df['pairing_id_orig'] = pairings_df['orig_index'].apply(lambda x: f\"P{x:06d}\")\n",
        "\n",
        "# Create a mapping flight_idx -> list of (pairing_orig_index, cost)\n",
        "flight_to_pairs = {}\n",
        "for f_str, plist in A_index.items():\n",
        "    f = int(f_str)\n",
        "    lst = []\n",
        "    for p in plist:\n",
        "        p_int = int(p)\n",
        "        if p_int < len(pairings_df):\n",
        "            lst.append((p_int, float(pairings_df.loc[p_int, 'cost'])))\n",
        "    # sort by cost\n",
        "    lst.sort(key=lambda x: x[1])\n",
        "    flight_to_pairs[f] = lst\n",
        "\n",
        "# pruning + solver loop\n",
        "from importlib import import_module\n",
        "# try to import your solver module (allow either 1662 or solver_1662)\n",
        "solver_module = None\n",
        "for modname in (\"solver_1662\", \"1662\", \"solver_1662_py\", \"solver1662\"):\n",
        "    try:\n",
        "        solver_module = import_module(modname)\n",
        "        break\n",
        "    except Exception:\n",
        "        pass\n",
        "if solver_module is None:\n",
        "    # fallback: try to import by filename\n",
        "    solver_path = os.path.join(\"/\", \"content\", \"1662.py\")\n",
        "    if os.path.exists(solver_path):\n",
        "        import importlib.util\n",
        "        spec = importlib.util.spec_from_file_location(\"solver_module_custom\", solver_path)\n",
        "        solver_module = importlib.util.module_from_spec(spec)\n",
        "        spec.loader.exec_module(solver_module)\n",
        "    else:\n",
        "        raise ImportError(\"Could not import your solver wrapper (1662.py). Make sure it's in /content or instance folder.\")\n",
        "\n",
        "solve_cpp = getattr(solver_module, \"solve_cpp\", None)\n",
        "if solve_cpp is None:\n",
        "    raise AttributeError(\"solve_cpp function not found in the solver module. Check 1662.py or solver_1662.py\")\n",
        "\n",
        "# function to write pruned CSVs for a given K\n",
        "def write_pruned_files(K):\n",
        "    # pick keep set: union over top-K per flight\n",
        "    keep_set = set()\n",
        "    for f, lst in flight_to_pairs.items():\n",
        "        topk = [p for p,_ in lst[:K]]\n",
        "        keep_set.update(topk)\n",
        "    keep_list = sorted(list(keep_set))\n",
        "    pruned_df = pairings_df.loc[keep_list].reset_index(drop=True).copy()\n",
        "    # reassign new pairing ids (0..)\n",
        "    # NOTE: use list comprehension to avoid Index.apply issue\n",
        "    pruned_df = pruned_df.reset_index(drop=True)\n",
        "    pruned_df['pairing_id'] = [f\"P{idx:06d}\" for idx in range(len(pruned_df))]\n",
        "\n",
        "    # write pairings.csv (fields solver expects: pairing_id, base, duration_hr, cost)\n",
        "    pairings_csv = os.path.join(INSTANCE_DIR, \"pairings.csv\")\n",
        "    if 'start_base' in pruned_df.columns:\n",
        "        base_col = 'start_base'\n",
        "    elif 'base' in pruned_df.columns:\n",
        "        base_col = 'base'\n",
        "    else:\n",
        "        # try to infer from first flight in flights_idx\n",
        "        def infer_base(row):\n",
        "            if isinstance(row.get('flights_idx'), (list,tuple)):\n",
        "                first_idx = int(row['flights_idx'][0])\n",
        "                return flights_df.loc[first_idx, 'dep_airport']\n",
        "            return ''\n",
        "        pruned_df['start_base'] = pruned_df.apply(infer_base, axis=1)\n",
        "        base_col = 'start_base'\n",
        "\n",
        "    pruned_df['duration_hr'] = pruned_df.get('total_hours', pruned_df.get('duration_hr', 0.0))\n",
        "    pruned_df[['pairing_id', base_col, 'duration_hr', 'cost']].rename(columns={base_col:'base'}).to_csv(pairings_csv, index=False)\n",
        "\n",
        "    # build a_ip mapping using new pairing ids\n",
        "    orig_to_new = {orig: new for new, orig in enumerate(keep_list)}\n",
        "    rows = []\n",
        "    for f_idx, lst in flight_to_pairs.items():\n",
        "        for orig_p, _ in lst:\n",
        "            if orig_p in orig_to_new:\n",
        "                new_idx = orig_to_new[orig_p]\n",
        "                pid = f\"P{new_idx:06d}\"\n",
        "                rows.append({\"flight_idx\": int(f_idx), \"pairing_id\": pid})\n",
        "    a_ip_df = pd.DataFrame(rows)\n",
        "    a_ip_df.to_csv(os.path.join(INSTANCE_DIR, \"a_ip.csv\"), index=False)\n",
        "\n",
        "    # write flights_clean.csv and credit_clean.csv if not present (keep existing ones if present)\n",
        "    flights_clean_csv = os.path.join(INSTANCE_DIR, \"flights_clean.csv\")\n",
        "    if not os.path.exists(flights_clean_csv):\n",
        "        fd = flights_df.copy()\n",
        "        fd['flight_idx'] = fd.index.astype(int)\n",
        "        fd['dep_dt'] = fd['dep_dt'].astype(str)\n",
        "        fd['arr_dt'] = fd['arr_dt'].astype(str)\n",
        "        fd[['flight_idx','flight_id','dep_airport','arr_airport','dep_dt','arr_dt','duration_hr']].to_csv(flights_clean_csv, index=False)\n",
        "\n",
        "    # credit file: try to reuse credit_clean.csv if exists, else create a safe non-binding file\n",
        "    credit_clean = os.path.join(INSTANCE_DIR, \"credit_clean.csv\")\n",
        "    if not os.path.exists(credit_clean):\n",
        "        # give large credits (no binding) as fallback\n",
        "        bases_df = pd.read_csv(LIST_OF_BASES)\n",
        "        bases_df.columns = bases_df.columns.str.strip().str.lower()\n",
        "        base_names = bases_df['airport'].astype(str).str.strip().tolist()\n",
        "        credit_map = [{\"base\": b, \"allowed_credit\": 1e9} for b in base_names]\n",
        "        pd.DataFrame(credit_map).to_csv(credit_clean, index=False)\n",
        "\n",
        "    return pruned_df.shape[0], os.path.join(INSTANCE_DIR, \"pairings.csv\"), os.path.join(INSTANCE_DIR, \"a_ip.csv\")\n",
        "\n",
        "# Try K loop\n",
        "Ks = [25, 15, 10, 5]\n",
        "success = False\n",
        "last_exception = None\n",
        "\n",
        "for K in Ks:\n",
        "    print(f\"\\nTrying K = {K} per-flight pruning ...\")\n",
        "    n_pairs, pairings_csv, a_ip_csv = write_pruned_files(K)\n",
        "    print(\"Pruned pairings written:\", pairings_csv, \"count:\", n_pairs)\n",
        "    # call solver\n",
        "    try:\n",
        "        start = time.time()\n",
        "        result = solve_cpp(INSTANCE_DIR,\n",
        "                           pairings_file=\"pairings.csv\",\n",
        "                           a_ip_file=\"a_ip.csv\",\n",
        "                           flights_file=\"flights_clean.csv\",\n",
        "                           credit_file=\"credit_clean.csv\",\n",
        "                           time_limit=600,\n",
        "                           mip_gap=1e-4)\n",
        "        # solve_cpp may return different signatures; handle either (selected_df, obj) or other\n",
        "        if isinstance(result, tuple) and len(result) >= 2:\n",
        "            selected_df, obj = result[0], result[1]\n",
        "        else:\n",
        "            # some wrappers return only selected_df\n",
        "            selected_df = result\n",
        "            obj = None\n",
        "        elapsed = time.time() - start\n",
        "        print(f\"Solve succeeded with K={K} in {elapsed:.1f}s; selected rows = {len(selected_df)}; obj = {obj}\")\n",
        "        # save selected file (if dataframe)\n",
        "        try:\n",
        "            sel_out = os.path.join(INSTANCE_DIR, \"selected_pairings.csv\")\n",
        "            if hasattr(selected_df, \"to_csv\"):\n",
        "                selected_df.to_csv(sel_out, index=False)\n",
        "                print(\"Saved selected_pairings.csv to\", sel_out)\n",
        "        except Exception:\n",
        "            pass\n",
        "        success = True\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(\"Solver failed for K =\", K, \"with exception:\", type(e), str(e))\n",
        "        last_exception = e\n",
        "        # continue with smaller K\n",
        "\n",
        "if not success:\n",
        "    print(\"\\nAll pruning attempts failed. Last exception:\")\n",
        "    import traceback\n",
        "    traceback.print_exception(type(last_exception), last_exception, last_exception.__traceback__)\n",
        "    print(\"\\nYou can either upload a full Gurobi license (gurobi.lic) into Colab or try the CBC fallback. \"\n",
        "          \"Tell me if you want me to auto-run the CBC fallback now.\")\n",
        "else:\n",
        "    print(\"\\nDone. If solution found, check /content/instance1/selected_pairings.csv and solver logs above.\")\n"
      ],
      "metadata": {
        "id": "3ZNOmk4JOjIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: make sure solver_1662.py is in python path (it was uploaded). If not, copy it to instance dir.\n",
        "import shutil, os\n",
        "src = \"/content/1662.py\"\n",
        "dst_dir = INSTANCE_DIR # Use the already defined INSTANCE_DIR\n",
        "dst_file = os.path.join(dst_dir, \"solver_1662.py\") # Rename the file\n",
        "shutil.copyfile(src, dst_file)\n",
        "print(\"Copied 1662.py to\", dst_file)"
      ],
      "metadata": {
        "id": "maZ_tvCZPksy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: import the wrapper and run solve_cpp\n",
        "import sys\n",
        "sys.path.append(INSTANCE_DIR)\n",
        "from solver_1662 import solve_cpp, solve_crp   # the file you uploaded; uses gurobipy\n",
        "\n",
        "# run CPP (adjust time_limit / files if needed)\n",
        "selected_df, obj = solve_cpp(INSTANCE_DIR,\n",
        "                             pairings_file=\"pairings.csv\",\n",
        "                             a_ip_file=\"a_ip.csv\",\n",
        "                             flights_file=\"flights_clean.csv\",\n",
        "                             credit_file=\"credit_clean.csv\",\n",
        "                             time_limit=600,   # seconds\n",
        "                             mip_gap=1e-4)\n",
        "\n",
        "print(\"CPP returned objective:\", obj)\n",
        "print(\"Selected pairings count:\", len(selected_df))\n",
        "selected_out = os.path.join(INSTANCE_DIR, \"selected_pairings.csv\")\n",
        "selected_df.to_csv(selected_out, index=False)\n",
        "print(\"Saved selected pairings to:\", selected_out)"
      ],
      "metadata": {
        "id": "kukxeU26QGxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell A: Validate CPP solution\n",
        "import pandas as pd, os, json\n",
        "INSTANCE_DIR = \"/content/instance1\"\n",
        "\n",
        "# load relevant files\n",
        "pair_sel = pd.read_csv(os.path.join(INSTANCE_DIR, \"selected_pairings.csv\"))\n",
        "pairings = pd.read_csv(os.path.join(INSTANCE_DIR, \"pairings.csv\"))\n",
        "flights = pd.read_csv(os.path.join(INSTANCE_DIR, \"flights_clean.csv\"))\n",
        "a_ip = pd.read_csv(os.path.join(INSTANCE_DIR, \"a_ip.csv\"))\n",
        "credit = pd.read_csv(os.path.join(INSTANCE_DIR, \"credit_clean.csv\"))\n",
        "bases = pd.read_csv(os.path.join(INSTANCE_DIR, \"listOfBases.csv\"), header=0)\n",
        "bases.columns = bases.columns.str.strip().str.lower()\n",
        "\n",
        "# Map selected pairings (pairing_id matches pairings.csv)\n",
        "selected_ids = set(pair_sel['pairing_id'].astype(str).tolist())\n",
        "print(\"Selected pairing ids sample:\", list(selected_ids)[:8])\n",
        "\n",
        "# 1) Coverage: for each flight, count selected columns covering it\n",
        "cov = a_ip[a_ip['pairing_id'].isin(selected_ids)].groupby('flight_idx').size().reset_index(name='cover_count')\n",
        "# join to flights to find uncovered flights\n",
        "flights['flight_idx'] = flights['flight_idx'].astype(int)\n",
        "cov_full = flights.merge(cov, how='left', left_on='flight_idx', right_on='flight_idx').fillna(0)\n",
        "uncovered = cov_full[cov_full['cover_count'] < 1]\n",
        "print(\"Total flights:\", len(flights), \"Covered flights:\", len(flights)-len(uncovered), \"Uncovered flights:\", len(uncovered))\n",
        "if len(uncovered) > 0:\n",
        "    display(uncovered.head(10))\n",
        "\n",
        "# 2) Base credit usage: sum credit/duration of selected pairings per base\n",
        "# pairings.csv has columns pairing_id, base, duration_hr (or duration_hr field name)\n",
        "pairings_columns = pairings.columns.tolist()\n",
        "dur_col = 'duration_hr' if 'duration_hr' in pairings_columns else ('total_hours' if 'total_hours' in pairings_columns else None)\n",
        "if dur_col is None:\n",
        "    # try reading the original pickle\n",
        "    try:\n",
        "        import pickle\n",
        "        pr = pd.read_pickle(\"/content/pairings_instance1.pkl\")\n",
        "        pr['pairing_id'] = pr.index.astype(str).apply(lambda x:f\"P{int(x):06d}\")\n",
        "        pr_sel = pr[pr['pairing_id'].isin(selected_ids)]\n",
        "        base_usage = pr_sel.groupby('start_base')['total_hours'].sum().reset_index(name='used_credit')\n",
        "    except Exception as e:\n",
        "        print(\"Couldn't compute credit usage automatically; please ensure duration column exists.\")\n",
        "        base_usage = pd.DataFrame()\n",
        "else:\n",
        "    pr_sel = pairings[pairings['pairing_id'].isin(selected_ids)]\n",
        "    base_usage = pr_sel.groupby('base')[dur_col].sum().reset_index(name='used_credit')\n",
        "\n",
        "# Merge with allowed credit\n",
        "credit.columns = credit.columns.str.strip().str.lower()\n",
        "if 'base' in credit.columns and ('allowed_credit' in credit.columns or 'credit' in credit.columns):\n",
        "    allowed_col = 'allowed_credit' if 'allowed_credit' in credit.columns else 'credit'\n",
        "    credit_map = credit.set_index('base')[allowed_col].to_dict()\n",
        "    base_usage['allowed_credit'] = base_usage['base'].map(lambda b: float(credit_map.get(b, 1e12)))\n",
        "    base_usage['pct_used'] = base_usage['used_credit'] / base_usage['allowed_credit'] * 100.0\n",
        "else:\n",
        "    base_usage['allowed_credit'] = float(1e12)\n",
        "    base_usage['pct_used'] = 0.0\n",
        "\n",
        "print(\"\\nBase credit usage (sample):\")\n",
        "display(base_usage.sort_values('pct_used', ascending=False).head(10))\n",
        "\n",
        "# Save diagnostic CSVs\n",
        "base_usage.to_csv(os.path.join(INSTANCE_DIR, \"diagnostic_base_usage.csv\"), index=False)\n",
        "cov_full.to_csv(os.path.join(INSTANCE_DIR, \"diagnostic_flight_coverage.csv\"), index=False)\n",
        "print(\"Diagnostics saved to instance folder: diagnostic_base_usage.csv, diagnostic_flight_coverage.csv\")\n"
      ],
      "metadata": {
        "id": "63ux5i_bVdGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell A: compute IIS for the CRP model and print problem constraints in IIS\n",
        "import importlib.util, os, time, pandas as pd, traceback\n",
        "\n",
        "INSTANCE_DIR = \"/content/instance1\"\n",
        "solver_path = \"/content/1662.py\"\n",
        "\n",
        "# load the solver module so we can access solve_crp internals if needed\n",
        "spec = importlib.util.spec_from_file_location(\"solver_mod\", solver_path)\n",
        "solver_mod = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(solver_mod)\n",
        "solve_crp = getattr(solver_mod, \"solve_crp\", None)\n",
        "\n",
        "# We need the actual Gurobi model object to compute IIS.\n",
        "# If solve_crp constructs it internally, we will re-run a modified function that returns the model for diagnostics.\n",
        "# To avoid editing your file permanently, we will create a small wrapper that calls your function but intercepts the model\n",
        "# If your solve_crp does not expose the model, we will re-implement the key part: build the same CRP model from CSVs and compute IIS.\n",
        "\n",
        "# === Build the CRP model here for diagnostics (re-creating what solve_crp does) ===\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "\n",
        "# load required CSVs\n",
        "pairings_file = os.path.join(INSTANCE_DIR, \"selected_pairings.csv\")  # pairings chosen by CPP\n",
        "crew_file     = os.path.join(INSTANCE_DIR, \"crew.csv\")\n",
        "req_crew = 1  # same as used previously\n",
        "\n",
        "pairings = pd.read_csv(pairings_file)\n",
        "crew = pd.read_csv(crew_file)\n",
        "# ensure columns and types\n",
        "pairings['pairing_id'] = pairings['pairing_id'].astype(str)\n",
        "crew['crew_id'] = crew['crew_id'].astype(str)\n",
        "\n",
        "# we need durations per pairing (column names may vary)\n",
        "if 'duration_hr' in pairings.columns:\n",
        "    dur_col = 'duration_hr'\n",
        "elif 'total_hours' in pairings.columns:\n",
        "    dur_col = 'total_hours'\n",
        "else:\n",
        "    # try to compute from first_dep/last_arr if available\n",
        "    if 'first_dep' in pairings.columns and 'last_arr' in pairings.columns:\n",
        "        pairings['duration_hr'] = (pd.to_datetime(pairings['last_arr']) - pd.to_datetime(pairings['first_dep'])).dt.total_seconds()/3600.0\n",
        "        dur_col = 'duration_hr'\n",
        "    else:\n",
        "        raise RuntimeError(\"Cannot find pairing duration column in selected_pairings.csv\")\n",
        "\n",
        "# Build a simple CRP model consistent with your wrapper:\n",
        "# Variables y[(c,p)] binary if crew c assigned to pairing p\n",
        "model = gp.Model(\"CRP_diag\")\n",
        "model.setParam('OutputFlag', 0)\n",
        "y = {}\n",
        "for c in crew['crew_id']:\n",
        "    for p in pairings['pairing_id']:\n",
        "        y[(c,p)] = model.addVar(vtype=GRB.BINARY, name=f\"y_{c}_{p}\")\n",
        "\n",
        "# Each pairing p must have req_crew assigned:\n",
        "for p in pairings['pairing_id']:\n",
        "    model.addConstr(gp.quicksum(y[(c,p)] for c in crew['crew_id']) >= req_crew, name=f\"cover_{p}\")\n",
        "\n",
        "# Crew hours constraint: sum(duration_p * y_cp) <= max_hours_c\n",
        "# ensure crew has max_hours column\n",
        "if 'max_hours' not in crew.columns:\n",
        "    crew['max_hours'] = 40.0\n",
        "for _, row in crew.iterrows():\n",
        "    c = row['crew_id']\n",
        "    Hc = float(row.get('max_hours', 40.0))\n",
        "    model.addConstr(gp.quicksum(pairings.loc[pairings['pairing_id']==p, dur_col].values[0] * y[(c,p)] for p in pairings['pairing_id']) <= Hc, name=f\"hours_{c}\")\n",
        "\n",
        "# You may also have base/qualification constraints in your wrapper; include basic base matching:\n",
        "if 'base' in pairings.columns:\n",
        "    # crew should only be able to take pairings starting from their base (if wrapper enforces this)\n",
        "    if 'base' in crew.columns:\n",
        "        for _, r in crew.iterrows():\n",
        "            c = r['crew_id']; cbase = str(r['base']).strip()\n",
        "            for p, prow in pairings.iterrows():\n",
        "                pbase = str(prow['base']).strip()\n",
        "                if pbase != cbase:\n",
        "                    # forbid assignment (add constraint y_cp == 0)\n",
        "                    model.addConstr(y[(c, prow['pairing_id'])] == 0, name=f\"base_forbid_{c}_{prow['pairing_id']}\")\n",
        "\n",
        "# Now compute IIS\n",
        "model.update()\n",
        "print(\"Computing IIS (this may take a few seconds)...\")\n",
        "model.computeIIS()\n",
        "iiss = []\n",
        "# model.write will create a .ilp with IIS marked; but we can inspect constraints/vars with IIS flags\n",
        "for constr in model.getConstrs():\n",
        "    if constr.IISConstr:\n",
        "        iiss.append((\"constr\", constr.ConstrName))\n",
        "for var in model.getVars():\n",
        "    if var.IISLB or var.IISUB:\n",
        "        iiss.append((\"var\", var.VarName))\n",
        "print(\"IIS elements found (constraints/vars):\", len(iiss))\n",
        "# print a readable sample of IIS items\n",
        "for kind, name in iiss[:200]:\n",
        "    print(kind, \":\", name)\n",
        "# write IIS to file for inspection\n",
        "model.write(os.path.join(INSTANCE_DIR, \"crp_iis.ilp\"))\n",
        "print(\"IIS written to:\", os.path.join(INSTANCE_DIR, \"crp_iis.ilp\"))\n"
      ],
      "metadata": {
        "id": "rNoZT83FQHfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell B: temporarily relax max_hours and re-run solve_crp to check feasibility root cause\n",
        "import pandas as pd, os, importlib.util, time\n",
        "\n",
        "INSTANCE_DIR = \"/content/instance1\"\n",
        "crew_csv = os.path.join(INSTANCE_DIR, \"crew.csv\")\n",
        "crew_df = pd.read_csv(crew_csv, dtype=str)\n",
        "# ensure columns\n",
        "if 'crew_id' not in crew_df.columns:\n",
        "    raise RuntimeError(\"crew.csv missing crew_id\")\n",
        "if 'base' not in crew_df.columns:\n",
        "    crew_df['base'] = crew_df['crew_id'].apply(lambda x: str(x).split('_')[0] if '_' in x else '')\n",
        "\n",
        "crew_df['max_hours'] = 1e6  # relax hours massively\n",
        "crew_df.to_csv(crew_csv, index=False)\n",
        "print(\"Temporarily set max_hours=1e6 for all crew and wrote crew.csv\")\n",
        "\n",
        "# reload solver and call solve_crp()\n",
        "spec = importlib.util.spec_from_file_location(\"solver_mod\", \"/content/1662.py\")\n",
        "solver_mod = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(solver_mod)\n",
        "solve_crp = getattr(solver_mod, \"solve_crp\", None)\n",
        "start = time.time()\n",
        "try:\n",
        "    res = solve_crp(INSTANCE_DIR,\n",
        "                    selected_pairings_file=\"selected_pairings.csv\",\n",
        "                    crew_file=\"crew.csv\",\n",
        "                    req_crew=1,\n",
        "                    time_limit=300)\n",
        "    elapsed = time.time() - start\n",
        "    print(\"solve_crp returned after\", f\"{elapsed:.1f}s\")\n",
        "    if isinstance(res, tuple):\n",
        "        assign_df = res[0]\n",
        "    else:\n",
        "        assign_df = res\n",
        "    print(\"Assignments rows:\", len(assign_df))\n",
        "    assign_df.to_csv(os.path.join(INSTANCE_DIR, \"crew_assignments_relaxed_hours.csv\"), index=False)\n",
        "    print(\"Saved crew_assignments_relaxed_hours.csv\")\n",
        "except Exception as e:\n",
        "    print(\"solve_crp still failed after relaxing max_hours. Exception:\")\n",
        "    import traceback\n",
        "    traceback.print_exception(type(e), e, e.__traceback__)\n"
      ],
      "metadata": {
        "id": "C-5A8G7CZXZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell C: automatically add reserve crews at busiest base(s) and re-run with normal max_hours\n",
        "import pandas as pd, os, time, importlib.util\n",
        "INSTANCE_DIR = \"/content/instance1\"\n",
        "pairings = pd.read_csv(os.path.join(INSTANCE_DIR, \"selected_pairings.csv\"))\n",
        "bases_usage = pairings.groupby('base').size().reset_index(name='n_p')\n",
        "bases_usage = bases_usage.sort_values('n_p', ascending=False)\n",
        "top_base = bases_usage.iloc[0]['base']\n",
        "print(\"Top base (will add reserves):\", top_base)\n",
        "\n",
        "crew_csv = os.path.join(INSTANCE_DIR, \"crew.csv\")\n",
        "crew_df = pd.read_csv(crew_csv, dtype=str)\n",
        "max_existing = pd.to_numeric(crew_df['max_hours'], errors='coerce').fillna(40.0).max()\n",
        "# add R reserve crews\n",
        "R = 3\n",
        "new_rows = []\n",
        "start_id = len(crew_df)\n",
        "for i in range(R):\n",
        "    new_rows.append({\"crew_id\": f\"RES_{i+1:02d}\", \"base\": top_base, \"max_hours\": 40.0})\n",
        "crew_df = pd.concat([crew_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
        "crew_df.to_csv(crew_csv, index=False)\n",
        "print(\"Added\", R, \"reserve crews at base\", top_base, \"and wrote crew.csv\")\n",
        "\n",
        "# re-run solver\n",
        "spec = importlib.util.spec_from_file_location(\"solver_mod\", \"/content/1662.py\")\n",
        "solver_mod = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(solver_mod)\n",
        "solve_crp = getattr(solver_mod, \"solve_crp\", None)\n",
        "\n",
        "start = time.time()\n",
        "res = solve_crp(INSTANCE_DIR,\n",
        "                selected_pairings_file=\"selected_pairings.csv\",\n",
        "                crew_file=\"crew.csv\",\n",
        "                req_crew=1,\n",
        "                time_limit=300)\n",
        "elapsed = time.time() - start\n",
        "if isinstance(res, tuple):\n",
        "    assign_df = res[0]\n",
        "else:\n",
        "    assign_df = res\n",
        "assign_df.to_csv(os.path.join(INSTANCE_DIR, \"crew_assignments_added_reserve.csv\"), index=False)\n",
        "print(\"Re-run finished in\", f\"{elapsed:.1f}s\", \"Assignments saved to crew_assignments_added_reserve.csv\")\n"
      ],
      "metadata": {
        "id": "h6ApQ8llZdpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation cell\n",
        "import pandas as pd, os, numpy as np\n",
        "\n",
        "INSTANCE_DIR = \"/content/instance1\"\n",
        "assign_csv = os.path.join(INSTANCE_DIR, \"crew_assignments_added_reserve.csv\")\n",
        "pairings_csv = os.path.join(INSTANCE_DIR, \"selected_pairings.csv\")\n",
        "crew_csv = os.path.join(INSTANCE_DIR, \"crew.csv\")\n",
        "a_ip_csv = os.path.join(INSTANCE_DIR, \"a_ip.csv\")\n",
        "\n",
        "assign_df = pd.read_csv(assign_csv)\n",
        "pairings = pd.read_csv(pairings_csv)\n",
        "crew = pd.read_csv(crew_csv)\n",
        "a_ip = pd.read_csv(a_ip_csv)\n",
        "\n",
        "# normalize names/types\n",
        "assign_df['crew'] = assign_df['crew'].astype(str)\n",
        "assign_df['pairing'] = assign_df['pairing'].astype(str)\n",
        "pairings['pairing_id'] = pairings['pairing_id'].astype(str)\n",
        "\n",
        "# 1) Coverage: count how many crews were assigned to each pairing\n",
        "cov_count = assign_df.groupby('pairing').size().reset_index(name='assigned_crews')\n",
        "# join required crew (we used req_crew=1 earlier)\n",
        "cov_count['required_crews'] = 1\n",
        "# find under-covered pairings\n",
        "under = cov_count[cov_count['assigned_crews'] < cov_count['required_crews']]\n",
        "print(\"Pairings with insufficient crew (should be 0):\", len(under))\n",
        "if len(under) > 0:\n",
        "    display(under.head(20))\n",
        "\n",
        "# 2) Per-crew hours: sum durations per assigned pairing (pairings must have duration_hr or total_hours)\n",
        "dur_col = 'duration_hr' if 'duration_hr' in pairings.columns else ('total_hours' if 'total_hours' in pairings.columns else None)\n",
        "if dur_col is None:\n",
        "    # attempt to compute\n",
        "    if 'first_dep' in pairings.columns and 'last_arr' in pairings.columns:\n",
        "        pairings[dur_col] = (pd.to_datetime(pairings['last_arr']) - pd.to_datetime(pairings['first_dep'])).dt.total_seconds()/3600.0\n",
        "    else:\n",
        "        raise RuntimeError(\"Cannot find pairing duration column in selected_pairings.csv\")\n",
        "\n",
        "# merge durations into assignments\n",
        "assign_with_dur = assign_df.merge(pairings[['pairing_id', dur_col, 'base']], left_on='pairing', right_on='pairing_id', how='left')\n",
        "crew_hours = assign_with_dur.groupby('crew')[dur_col].sum().reset_index(name='assigned_hours')\n",
        "\n",
        "# merge max_hours from crew.csv if present\n",
        "if 'max_hours' in crew.columns:\n",
        "    crew_map = crew.set_index('crew_id')['max_hours'].to_dict()\n",
        "    crew_hours['max_hours'] = crew_hours['crew'].map(lambda x: float(crew_map.get(x, 40.0)))\n",
        "else:\n",
        "    crew_hours['max_hours'] = 40.0\n",
        "\n",
        "crew_hours['pct_used'] = crew_hours['assigned_hours'] / crew_hours['max_hours'] * 100.0\n",
        "print(\"\\nPer-crew hours summary (top 10 busiest):\")\n",
        "display(crew_hours.sort_values('assigned_hours', ascending=False).head(10))\n",
        "\n",
        "# 3) Per-base assignment counts\n",
        "assign_with_base = assign_with_dur.groupby('base').size().reset_index(name='assignments_count')\n",
        "print(\"\\nAssignments per base:\")\n",
        "display(assign_with_base.sort_values('assignments_count', ascending=False))\n",
        "\n",
        "# 4) Save diagnostics\n",
        "crew_hours.to_csv(os.path.join(INSTANCE_DIR, \"diag_crew_hours.csv\"), index=False)\n",
        "assign_with_base.to_csv(os.path.join(INSTANCE_DIR, \"diag_assign_per_base.csv\"), index=False)\n",
        "print(\"\\nDiagnostics saved: diag_crew_hours.csv, diag_assign_per_base.csv\")\n"
      ],
      "metadata": {
        "id": "jvgOUZTEbft2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Robust roster builder: handles missing first_dep/last_arr by reconstructing from pairings pickle + flights\n",
        "import os, pandas as pd, ast, json\n",
        "INSTANCE_DIR = \"/content/instance1\"\n",
        "\n",
        "# load files\n",
        "assign_path = os.path.join(INSTANCE_DIR, \"crew_assignments_added_reserve.csv\")\n",
        "pairings_csv = os.path.join(INSTANCE_DIR, \"selected_pairings.csv\")\n",
        "pairings_pkl = \"/content/pairings_instance1.pkl\"\n",
        "flights_pkl = \"/content/flights_instance1.pkl\"\n",
        "\n",
        "assign_df = pd.read_csv(assign_path)\n",
        "# try to load pairings CSV first\n",
        "if os.path.exists(pairings_csv):\n",
        "    pairings = pd.read_csv(pairings_csv)\n",
        "else:\n",
        "    pairings = None\n",
        "\n",
        "# load original pairing pickle if available (more detailed)\n",
        "orig_pairings = None\n",
        "if os.path.exists(pairings_pkl):\n",
        "    try:\n",
        "        orig_pairings = pd.read_pickle(pairings_pkl)\n",
        "    except Exception:\n",
        "        orig_pairings = None\n",
        "\n",
        "# load flights for timestamp reconstruction\n",
        "if os.path.exists(flights_pkl):\n",
        "    flights_df = pd.read_pickle(flights_pkl)\n",
        "else:\n",
        "    # try flights_clean.csv fallback\n",
        "    flights_clean_csv = os.path.join(INSTANCE_DIR, \"flights_clean.csv\")\n",
        "    flights_df = pd.read_csv(flights_clean_csv)\n",
        "    flights_df['dep_dt'] = pd.to_datetime(flights_df['dep_dt'])\n",
        "    flights_df['arr_dt'] = pd.to_datetime(flights_df['arr_dt'])\n",
        "\n",
        "# If the pairings CSV already contains first_dep/last_arr/duration_hr, use it; otherwise reconstruct\n",
        "need_recompute = True\n",
        "if pairings is not None:\n",
        "    cols = [c.lower() for c in pairings.columns]\n",
        "    if 'first_dep' in cols or 'first_dep' in pairings.columns:\n",
        "        need_recompute = False\n",
        "    if 'last_arr' in cols or 'last_arr' in pairings.columns:\n",
        "        need_recompute = need_recompute and False\n",
        "    # but ensure duration exists\n",
        "    if 'duration_hr' in cols or 'total_hours' in cols:\n",
        "        pass\n",
        "\n",
        "# Build a canonical pairings_df that contains pairing_id, flights_idx (list[int]), first_dep, last_arr, duration_hr, base\n",
        "def extract_pairings_dataframe():\n",
        "    # prefer original pickle for flights_idx field (more reliable)\n",
        "    if orig_pairings is not None:\n",
        "        p = orig_pairings.reset_index(drop=True).copy()\n",
        "        # ensure pairing_id aligns with selected_pairings ids: Selected used P000000... mapping earlier\n",
        "        # Create pairing_id format consistent with pruned selected_pairings (if matching by index)\n",
        "        p['pairing_id'] = p.index.map(lambda x: f\"P{int(x):06d}\")\n",
        "        # flights_idx may be list already; if string, parse\n",
        "        if 'flights_idx' in p.columns:\n",
        "            def parse_list(x):\n",
        "                if isinstance(x, (list,tuple)):\n",
        "                    return [int(i) for i in x]\n",
        "                s = str(x)\n",
        "                # try JSON\n",
        "                try:\n",
        "                    v = json.loads(s)\n",
        "                    return [int(i) for i in v]\n",
        "                except:\n",
        "                    pass\n",
        "                # try ast\n",
        "                try:\n",
        "                    v = ast.literal_eval(s)\n",
        "                    return [int(i) for i in v]\n",
        "                except:\n",
        "                    return []\n",
        "            p['flights_idx'] = p['flights_idx'].apply(parse_list)\n",
        "        else:\n",
        "            p['flights_idx'] = [[] for _ in range(len(p))]\n",
        "        # compute first_dep/last_arr/duration\n",
        "        def compute_times(flist):\n",
        "            if not flist:\n",
        "                return (pd.NaT, pd.NaT, 0.0)\n",
        "            # map flight idx to datetimes in flights_df\n",
        "            deps = []\n",
        "            arrs = []\n",
        "            for fi in flist:\n",
        "                # some flights_df index may not match numeric id; try locate by 'flight_idx' or by position\n",
        "                if 'flight_idx' in flights_df.columns:\n",
        "                    row = flights_df[flights_df['flight_idx']==int(fi)]\n",
        "                    if row.shape[0] == 0:\n",
        "                        # try positional\n",
        "                        try:\n",
        "                            row = flights_df.iloc[int(fi)]\n",
        "                            deps.append(pd.to_datetime(row['dep_dt']))\n",
        "                            arrs.append(pd.to_datetime(row['arr_dt']))\n",
        "                            continue\n",
        "                        except Exception:\n",
        "                            continue\n",
        "                    deps.append(pd.to_datetime(row.iloc[0]['dep_dt']))\n",
        "                    arrs.append(pd.to_datetime(row.iloc[0]['arr_dt']))\n",
        "                else:\n",
        "                    try:\n",
        "                        row = flights_df.iloc[int(fi)]\n",
        "                        deps.append(pd.to_datetime(row['dep_dt']))\n",
        "                        arrs.append(pd.to_datetime(row['arr_dt']))\n",
        "                    except Exception:\n",
        "                        continue\n",
        "            if not deps:\n",
        "                return (pd.NaT, pd.NaT, 0.0)\n",
        "            first_dep = min(deps)\n",
        "            last_arr = max(arrs)\n",
        "            duration_hr = (last_arr - first_dep).total_seconds() / 3600.0\n",
        "            return (first_dep, last_arr, duration_hr)\n",
        "        computed = p['flights_idx'].apply(compute_times)\n",
        "        p['first_dep'] = computed.apply(lambda t: t[0])\n",
        "        p['last_arr'] = computed.apply(lambda t: t[1])\n",
        "        p['duration_hr'] = computed.apply(lambda t: t[2])\n",
        "        # ensure base column exists\n",
        "        if 'start_base' in p.columns:\n",
        "            p['base'] = p['start_base'].astype(str)\n",
        "        elif 'base' in p.columns:\n",
        "            p['base'] = p['base'].astype(str)\n",
        "        else:\n",
        "            p['base'] = p['flights_idx'].apply(lambda lst: flights_df.loc[flights_df['flight_idx']==int(lst[0]), 'dep_airport'].values[0] if lst else '')\n",
        "        # stringify first_dep/last_arr\n",
        "        p['first_dep'] = pd.to_datetime(p['first_dep'])\n",
        "        p['last_arr'] = pd.to_datetime(p['last_arr'])\n",
        "        return p[['pairing_id','flights_idx','base','first_dep','last_arr','duration_hr']].copy()\n",
        "    else:\n",
        "        # fallback: read pairings CSV and try to reconstruct using mapping in a_ip.json\n",
        "        if pairings is None:\n",
        "            raise RuntimeError(\"No pairing data available (neither pickle nor CSV). Cannot reconstruct roster.\")\n",
        "        p = pairings.copy()\n",
        "        # Ensure pairing_id exists\n",
        "        if 'pairing_id' not in p.columns:\n",
        "            p = p.reset_index().rename(columns={'index':'pairing_id'})\n",
        "            p['pairing_id'] = p['pairing_id'].astype(str).apply(lambda x: f\"P{int(x):06d}\")\n",
        "        # try to create flights_idx mapping from a_ip mapping\n",
        "        a_ip = pd.read_csv(os.path.join(INSTANCE_DIR, \"a_ip.csv\"))\n",
        "        # build reverse map pairing_id -> list flight_idx\n",
        "        map_p_to_f = a_ip.groupby('pairing_id')['flight_idx'].apply(lambda s: [int(x) for x in s.tolist()]).to_dict()\n",
        "        p['flights_idx'] = p['pairing_id'].map(lambda pid: map_p_to_f.get(pid, []))\n",
        "        # compute times same as above\n",
        "        def compute_times2(lst):\n",
        "            if not lst:\n",
        "                return (pd.NaT, pd.NaT, 0.0)\n",
        "            deps, arrs = [], []\n",
        "            for fi in lst:\n",
        "                try:\n",
        "                    row = flights_df[flights_df['flight_idx']==int(fi)]\n",
        "                    if row.shape[0] > 0:\n",
        "                        deps.append(pd.to_datetime(row.iloc[0]['dep_dt']))\n",
        "                        arrs.append(pd.to_datetime(row.iloc[0]['arr_dt']))\n",
        "                    else:\n",
        "                        r2 = flights_df.iloc[int(fi)]\n",
        "                        deps.append(pd.to_datetime(r2['dep_dt']))\n",
        "                        arrs.append(pd.to_datetime(r2['arr_dt']))\n",
        "                except Exception:\n",
        "                    continue\n",
        "            if not deps:\n",
        "                return (pd.NaT, pd.NaT, 0.0)\n",
        "            first_dep = min(deps); last_arr = max(arrs)\n",
        "            return (first_dep, last_arr, (last_arr-first_dep).total_seconds()/3600.0)\n",
        "        computed = p['flights_idx'].apply(compute_times2)\n",
        "        p['first_dep'] = computed.apply(lambda t: t[0])\n",
        "        p['last_arr'] = computed.apply(lambda t: t[1])\n",
        "        p['duration_hr'] = computed.apply(lambda t: t[2])\n",
        "        if 'base' not in p.columns:\n",
        "            p['base'] = p['flights_idx'].apply(lambda lst: flights_df.loc[flights_df['flight_idx']==int(lst[0]), 'dep_airport'].values[0] if lst else '')\n",
        "        p['first_dep'] = pd.to_datetime(p['first_dep'])\n",
        "        p['last_arr'] = pd.to_datetime(p['last_arr'])\n",
        "        return p[['pairing_id','flights_idx','base','first_dep','last_arr','duration_hr']].copy()\n",
        "\n",
        "pairings_full = extract_pairings_dataframe()\n",
        "\n",
        "# Merge assignment with pairing timestamps\n",
        "assign_df['pairing'] = assign_df['pairing'].astype(str)\n",
        "merged = assign_df.merge(pairings_full, left_on='pairing', right_on='pairing_id', how='left')\n",
        "# re-order and select useful columns\n",
        "roster = merged[['crew','pairing','base','duration_hr','first_dep','last_arr']].copy()\n",
        "roster = roster.sort_values(['crew','first_dep']).reset_index(drop=True)\n",
        "\n",
        "# Save roster CSV\n",
        "out_csv = os.path.join(INSTANCE_DIR, \"rosters_per_crew.csv\")\n",
        "roster.to_csv(out_csv, index=False)\n",
        "print(\"Wrote roster CSV:\", out_csv)\n",
        "display(roster.head(30))"
      ],
      "metadata": {
        "id": "8EaSQQvrblWE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}